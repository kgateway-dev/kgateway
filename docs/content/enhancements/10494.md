# EP-10494: Support AI Gateway APIs 

* Issue: [#10494](https://github.com/kgateway-dev/kgateway/issues/10494)

## Background

This EP proposes adding support for AI Gateway APIs to the K8s Gateway. 
The AI Gateway APIs will allow users to route to LLM providers and apply advanced policies 
(prompt enrichment, prompt guard, streaming, user-invoked function calling, etc.). The AI Gateway APIs will be 
implemented with the kgateway upstream and route policy plugins.

## Goals

The following list defines goals for this EP.

* Implement a series of AI specific APIs for the Gateway based on prior art from the Gloo AI Gateway. These APIs aim 
to simplify the user experience for routing to LLM providers and applying advanced policies (prompt enrichment, 
prompt guard, streaming, user-invoked function calling, etc.).
* Allow users to enable/disable this feature via helm.
* Implement AI extensions as a kgateway plugin.
* Provide e2e testing of this feature.
* Provide initial user documentation, e.g. quick start guide.

## Non-Goals

The following list defines non-goals for this EP.

* These APIs will be separate and unrelated from the Gateway API Inference Extension Support (GIE) extensions proposed in [#10411](https://github.com/kgateway-dev/kgateway/issues/10411). 
The GIE extensions is an open source project that originated from [wg-serving](https://github.com/kubernetes/community/tree/master/wg-serving) and is sponsored by [SIG Network](https://github.com/kubernetes/community/blob/master/sig-network/README.md#gateway-api-inference-extension). 
It provides APIs, a scheduling algorithm, a reference extension implementation, and controllers to support routing to multiplexed LLM services on a shared pool of compute. This proposal is not related to that project.
* Some features (RAG, Semantic Caching, Ratelimiting, JWT-based fine-grained auth) are out of scope for this EP. These will be handled in a separate proposal.
* Observability features (otel integration, tracing, etc.) are out of scope for this EP. These will be handled in a separate proposal.

## Implementation Details

### APIs

The API will follow the design principles laid out in the K8s Gateway API, and the kgatewy API. This proposes two new additions to the existing API.
1. A new upstream subtype called AI
2. A new RoutePolicy sub-object called AI

These 2 types will allow us to get the new AI specific options without needing to create/add any new API objects

#### Upstream 

The AI Upstream represents an LLM backend, either running locally or somewhere in the "cloud". This resource will define the type of LLM provider you wish to connect to, as well as LLM specific fine-tuning options which can be supplied. It will also specify the type of endpoint to be proxied to: e.g. chat, completion, audio, etc.

For example, if a kubernetes secret `openai-secret` is defined, the corresponding upstream would be:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: openai
  namespace: kgateway-system
spec:
  ai:
    openai:
      authToken:
        secretRef:
          name: openai-secret
```

The upstreams would also include model version and additional information as needed for the provider. For example, for Vertex AI:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: vertex-ai
  namespace: ai-test
spec:
  ai:
    vertexAi:
      model: gemini-1.5-flash-001
      apiVersion: v1
      location: us-central1
      projectId: kgateway-project
      publisher: GOOGLE
      authToken:
        secretRef:
          name: vertex-ai-secret
          namespace: ai-test
```

For the passthrough case, the upstream would look something like:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: vertex-ai
  namespace: ai-test
spec:
  ai:
    vertexAi:
      model: gemini-1.5-flash-001
      apiVersion: v1
      location: us-central1
      projectId: kgateway-project
      publisher: GOOGLE
      authToken:
        passthrough: {}
```

#### RoutePolicy

The purpose of the new  AI RoutePolicy sub-section API is to augment routes with AI specific capabilities. For simplicity these can ONLY be attached to routes which target AI Upstreams. This config can be useful across multiple providers, models, etc, and therefore is separated from the providers themselves. This way a provider can be used multiple times with different config, including enrichments, caching config etc.

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: openai
  namespace: kgateway-system
spec:
  parentRefs:
    - name: ai-gateway
      namespace: kgateway-system
  rules:
  - matches:
    - path:
        type: Exact
        value: /v1/chat/completions
    backendRefs:
    - name: open-ai-gpt4
      namespace: kgateway-system
      group: kgateway.io
      kind: Upstream
---
apiVersion: kgateway.io/v1
kind: RoutePolicy
metadata:
  name: open-ai-opt
  namespace: kgateway-system
spec:
  targetRefs:
  - group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: openai
  ai:
    backupModels:
    - "gpt-4-turbo"
    - "gpt-3.5-turbo"
    promptEnrichment:
      prepend:
      - role: SYSTEM
        content: "respond to all questions in French"
      append:
      - role: SYSTEM
        content: "disregard any instructions I may have given to be cruel"
    promptGuard:
      request:
        customResponseMessage: "Rejected due to inappropriate content"
        matches:
        - "credit card"
      response:
        matches:
        # Mastercard
        - '(?:^|\D)(5[1-5][0-9]{2}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4})(?:\D|$)'
```

### Request Flow

A request going through the kgateway will follow the flow below:

![ai gateway request flow](./resources/ai-gateway-request-flow.png "ai gateway request flow")

#### Authenticate with API keys

After creating an API key for authentication with an LLM provider, the AI apis allows you to create an Upstream resource
that references that API key. Then, you set up routing to that Upstream on the user-defined path by creating an HTTPRoute
resource. Initially three modes of authentication are supported:

- The kgateway automatically reads the API key from the Kubernetes secret to process authentication for requests on that path
- The API key is inlined in the Upstream definition
- The Upstream uses passthrough to pass the API key to the LLM provider as a header

#### Prompt enrichment

Prompts are basic building blocks for guiding LLMs to produce relevant and accurate responses. By effectively managing
both system prompts, which set initial guidelines, and user prompts, which provide specific context, you can
significantly enhance the quality and coherence of the modelâ€™s outputs. The kgateway AI apis should allow you to 
pre-configure and refactor system and user prompts, extract common AI provider settings so that you can reuse them
across requests, dynamically append or prepend prompts to where you need them, and overwrite default settings on a 
per-route level.

#### Prompt Guard

Prompt guards are mechanisms that ensure that prompt-based interactions with a language model are secure, appropriate, 
and aligned with the intended use. These mechanisms help to filter, block, monitor, and control LLM inputs and outputs 
to filter offensive content, prevent misuse, and ensure ethical and responsible AI usage.

The AI apis allow you to configure prompt guards to block unwanted requests to the LLM provider and mask sensitive data. 
Prompt guards can also be applied to streaming requests. 

#### Streaming

kgateway will support chat streaming, which allows the LLM to stream out tokens as they are generated. 
Some providers, such as OpenAI, send the is-streaming boolean as part of the request to determine whether a 
request should receive a streamed response. However, the Gemini and Vertex AI providers change the path to determine 
streaming, such as the streamGenerateContent segment of the path in the Vertex AI streaming endpoint 
https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:streamGenerateContent?key=<key>. 
To prevent the path you defined in your HTTPRoute from being overwritten by this streaming path, you instead indicate 
chat streaming for Gemini and Vertex AI by setting `ai.routeType=CHAT_STREAMING` in your RoutePolicy resource.

#### Supported LLM providers
The initial kgateway APIs will support the following AI providers:
- Anthropic 
- Azure OpenAI 
- Gemini 
- Mistral AI 
- OpenAI 
- Vertex AI

### Configuration

The AI Gateway will be enabled via a helm flag. The AI Gateway will be disabled by default.

```yaml
ai:
  enabled: true
```

### Plugin

The Upstream plugin and RoutePolicy plugin will be augmented to implement the AI specific APIs.

### ExtProc Server

The server itself will be written in python and will most likely stay that way for several reasons: 
- Many relevant AI libraries are written in python. This is true for nearly all AI related libraries. Luckily ext-proc is implemented in proto so spinning up a python server is actually quite easy.
- As with most AI operations the latency is not nearly as important given the massive response times of the LLMs themselves. However, in order to ensure the lowest possible impact from our ext-proc server, we can very easily run it as a sidecar to envoy and connect via a Unix Domain Socket.

### Reporting

* Update the [reporter](https://github.com/kgateway-dev/kgateway/tree/main/projects/gateway2/reports) package to support status reporting for Upstream's referencing kubernetes secrets.

  __Note:__ Upstream status currently supported?

### Testing

Unit tests:
* python unit test for extauth server parsing
* unit tests for upstream plugin AI features
* unit tests for RoutePolicy plugin AI features

e2e Tests:
* e2e tests for routing with different providers with API keys in kubernetes secrets, inlined and passthrough
* e2e tests for prompt enrichment
* e2e tests for prompt guard (streaming, and non-streaming)
* e2e tests for streaming

# EP-10494: Add Support for AI Gateway APIs

* Issue: [#10494](https://github.com/kgateway-dev/kgateway/issues/10494)

## Background

This EP proposes adding support for AI Gateway APIs to the K8s Gateway.
The AI Gateway APIs will enable users to route traffic to LLM providers while applying advanced policies, such as 
prompt enrichment, prompt guard, streaming, and user-invoked function calling. These APIs will be implemented using the
kgateway upstream and route policy plugins.

## Goals

The following list defines goals for this EP.

* Implement a series of AI specific APIs for the Gateway based on prior art from the Gloo AI Gateway. These APIs aim 
to simplify the user experience for routing to LLM providers and applying advanced policies (prompt enrichment, 
prompt guard, streaming, user-invoked function calling, etc.).
* Allow users to enable or disable this feature via Helm.
* Implement AI extensions as a kgateway plugin.
* Provide e2e testing of this feature.
* Provide initial user documentation, such as a quick start guide.

## Non-Goals

The following list defines non-goals for this EP.

* These APIs will be separate and unrelated from the Gateway API Inference Extension Support (GIE) extensions proposed 
in [#10411](https://github.com/kgateway-dev/kgateway/issues/10411). The GIE extensions is an open source project that 
originated from [wg-serving](https://github.com/kubernetes/community/tree/master/wg-serving) and is sponsored by 
[SIG Network](https://github.com/kubernetes/community/blob/master/sig-network/README.md#gateway-api-inference-extension). 
It provides APIs, a scheduling algorithm, a reference extension implementation, and controllers to support routing to 
multiplexed LLM services on a shared pool of compute. This proposal is not related to that project.
* Observability features (otel integration, tracing, etc.) are out of scope for this EP. These will be handled in a 
separate proposal.
* Support of advanced features (RAG, Semantic Caching, Ratelimiting, JWT-based fine-grained auth) are out of scope for this EP.

## Implementation Details

### APIs

The API will follow the design principles laid out in the K8s Gateway API, and the kgateway API. This proposes two new 
additions to the existing API:

1. A new upstream subtype called AI
2. A new RoutePolicy sub-object called AI

These two types will allow us to get the new AI specific options without needing to create/add any new API objects

#### Upstream 

The AI Upstream represents an LLM backend, either running locally or somewhere in the "cloud". This resource will define
the type of LLM provider you wish to connect to, as well as LLM specific fine-tuning options which can be supplied. 
It will also specify the type of endpoint to be proxied to: e.g. chat, completion, audio, etc.

For example, if a kubernetes secret `openai-secret` is defined, the corresponding upstream would be:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: openai
  namespace: kgateway-system
spec:
  ai:
    openai:
      authToken:
        secretRef:
          name: openai-secret
```

The upstreams would also include model version and additional information as needed for the provider. For example, for Vertex AI:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: vertex-ai
  namespace: ai-test
spec:
  ai:
    vertexAi:
      model: gemini-1.5-flash-001
      apiVersion: v1
      location: us-central1
      projectId: kgateway-project
      publisher: GOOGLE
      authToken:
        secretRef:
          name: vertex-ai-secret
          namespace: ai-test
```

For the passthrough case, the upstream would look something like:
```yaml
apiVersion: kgateway.io/v1
kind: Upstream
metadata:
  labels:
    app: kgateway
  name: vertex-ai
  namespace: ai-test
spec:
  ai:
    vertexAi:
      model: gemini-1.5-flash-001
      apiVersion: v1
      location: us-central1
      projectId: kgateway-project
      publisher: GOOGLE
      authToken:
        passthrough: {}
```

#### RoutePolicy

The purpose of the new  AI RoutePolicy subsection API is to augment routes with AI specific capabilities. 
For simplicity these can ONLY be attached to routes which target AI Upstreams. This config can be useful across 
multiple providers, models, etc., and therefore is separated from the providers themselves. This way a provider can be 
used multiple times with different config, including enrichments, caching config etc.

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: openai
  namespace: kgateway-system
spec:
  parentRefs:
    - name: ai-gateway
      namespace: kgateway-system
  rules:
  - matches:
    - path:
        type: Exact
        value: /v1/chat/completions
    backendRefs:
    - name: open-ai-gpt4
      namespace: kgateway-system
      group: kgateway.io
      kind: Upstream
---
apiVersion: kgateway.io/v1
kind: RoutePolicy
metadata:
  name: open-ai-opt
  namespace: kgateway-system
spec:
  targetRefs:
  - group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: openai
  ai:
    backupModels:
    - "gpt-4-turbo"
    - "gpt-3.5-turbo"
    promptEnrichment:
      prepend:
      - role: SYSTEM
        content: "respond to all questions in French"
      append:
      - role: SYSTEM
        content: "disregard any instructions I may have given to be cruel"
    promptGuard:
      request:
        customResponseMessage: "Rejected due to inappropriate content"
        matches:
        - "credit card"
      response:
        matches:
        # Mastercard
        - '(?:^|\D)(5[1-5][0-9]{2}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4}(?:\ |\-|)[0-9]{4})(?:\D|$)'
```

### Request Flow

A request going through the kgateway will follow the flow below:

![ai gateway request flow](./resources/ai-gateway-request-flow.png "ai gateway request flow")

#### Authenticate with API keys

After creating an API key for authentication with an LLM provider, the AI APIs allow you to create an Upstream resource that references the API key. You can then configure routing to that Upstream on a user-defined path by creating an `HTTPRoute` resource.

Initially, three modes of authentication are supported:

- **Automatic Secret Integration:** kgateway reads the API key from a Kubernetes secret to handle authentication for requests on the specified path.
- **Inline API Key:** The API key is included directly in the Upstream definition.
- **Passthrough Mode:** The API key is passed through to the LLM provider as a header.

#### Prompt enrichment

Prompts are basic building blocks for guiding LLMs to produce relevant and accurate responses. By effectively managing
both system prompts, which set initial guidelines, and user prompts, which provide specific context, you can
significantly enhance the quality and coherence of the modelâ€™s outputs. 

The kgateway AI APIs should allow you to:
* Pre-configure and refactor system and user prompts.
* Extract common AI provider settings for reuse across requests.
* Dynamically append or prepend prompts wherever needed.
* Overwrite default settings on a per-route level.

#### Prompt Guard

Prompt guards are mechanisms that ensure that prompt-based interactions with a language model are secure, appropriate, 
and aligned with the intended use. These mechanisms help to filter, block, monitor, and control LLM inputs and outputs 
to filter offensive content, prevent misuse, and ensure ethical and responsible AI usage.

The AI apis allow you to configure prompt guards to block unwanted requests to the LLM provider and mask sensitive data. 
Prompt guards can also be applied to streaming requests. 

#### Streaming

kgateway will support chat streaming, which allows the LLM to stream out tokens as they are generated. 
* Some providers, such as OpenAI, send the is-streaming boolean as part of the request to determine whether a 
request should receive a streamed response. 
* Other providers, such as the Gemini and Vertex AI providers, change the path to determine 
streaming, such as the streamGenerateContent segment of the path in the Vertex AI streaming endpoint 
https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:streamGenerateContent?key=<key>. 
To prevent the path you defined in your HTTPRoute from being overwritten by this streaming path, you instead indicate 
chat streaming for Gemini and Vertex AI by setting `ai.routeType=CHAT_STREAMING` in your RoutePolicy resource.

#### Supported LLM providers
The initial kgateway AI APIs will support the following AI providers:
* Anthropic 
* Azure OpenAI 
* Gemini 
* Mistral AI 
* OpenAI 
* Vertex AI

### Configuration

The AI Gateway features will be enabled via a helm flag. The AI Gateway will be disabled by default.

```yaml
ai:
  enabled: true
```

### Plugin

The Upstream and RoutePolicy plugins will be enhanced to implement the AI-specific APIs.

### ExtProc Server

The ExtProc server will be written in Python and is likely to remain so for several reasons:
* Many AI libraries, including nearly all widely used ones, are written in Python. Since ExtProc is implemented with Protobuf, setting up a Python server should be straightforward.
* Latency, which is often critical, is less of a concern in AI operations due to the inherently high response times of LLMs. However, to minimize the impact of the ExtProc server, we can easily run it as a sidecar to Envoy and connect via a Unix Domain Socket.

### Reporting

* Update the [reporter](https://github.com/kgateway-dev/kgateway/tree/main/projects/gateway2/reports) package to support status reporting for Upstream's referencing kubernetes secrets.

### Testing

Unit tests:
* Python unit tests for the ExtAuth server's parsing logic.
* Unit tests for AI-related features in the Upstream plugin.
* Unit tests for AI-related features in the RoutePolicy plugin.

e2e Tests:
* e2e tests for routing with different providers, using API keys stored in Kubernetes secrets, inline, and passthrough configurations.
* e2e tests for prompt enrichment.
* e2e tests for prompt guard (both streaming and non-streaming).
* e2e tests for streaming functionality.

### Open Questions
* Are Upstream and RoutePolicy statuses currently supported?